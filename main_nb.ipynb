{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda08ea4702d26c4a568fe3dcda8da50807",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "' ##################### Main script #########################\\n\\nThis script has been designed to perform training with several \\nvariations of the models of the nnUNet framework, including a \\n3-fold cross validation of our results. The structure of the \\nscript is as follows:\\n\\n    1 - Selection of the training, validation and testing sets.\\n    This is done for each fold of the cross validation.\\n    2 - Generation of the .json file, necessary to perform \\n    training and validation.\\n    3 - Preprocessing of the dataset.\\n    4 - Training routine.\\n    5 - Inference and evaluation of results.\\n    6 - At the end of the cross-validation, we compute evalua-\\n    tion metrics across all folds.\\n\\nWe repeat these steps for each of the modifications that we add\\nto the nnUNet framewrok. This includes changes in (1) data aug-\\nmentation, (2) resolution of dataset, and (3) network depth.\\n\\n'"
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "\"\"\" ##################### Main script #########################\n",
    "\n",
    "This script has been designed to perform training with several \n",
    "variations of the models of the nnUNet framework, including a \n",
    "3-fold cross validation of our results. The structure of the \n",
    "script is as follows:\n",
    "\n",
    "    1 - Selection of the training, validation and testing sets.\n",
    "    This is done for each fold of the cross validation.\n",
    "    2 - Generation of the .json file, necessary to perform \n",
    "    training and validation.\n",
    "    3 - Preprocessing of the dataset.\n",
    "    4 - Training routine.\n",
    "    5 - Inference and evaluation of results.\n",
    "    6 - At the end of the cross-validation, we compute evalua-\n",
    "    tion metrics across all folds.\n",
    "\n",
    "We repeat these steps for each of the modifications that we add\n",
    "to the nnUNet framewrok. This includes changes in (1) data aug-\n",
    "mentation, (2) resolution of dataset, and (3) network depth.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from batchgenerators.utilities.file_and_folder_operations import maybe_mkdir_p, join\n",
    "\n",
    "import nibabel as nib\n",
    "from nibabel.testing import data_path\n",
    "\n",
    "import json\n",
    "\n",
    "from evaluation_metrics import eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the database folders (images and labels)\n",
    "\n",
    "# nnunet_dir = \"/home/perecanals/nnunet_env/nnUNet/nnunet\"\n",
    "nnunet_dir = \"/Users/pere/opt/anaconda3/envs/nnunet_env/nnUNet/nnunet\"\n",
    "\n",
    "path_images_base = join(nnunet_dir, \"nnUNet_base/nnUNet_raw/Task00_grid/database_images\")\n",
    "path_labels_base = join(nnunet_dir, \"nnUNet_base/nnUNet_raw/Task00_grid/database_labels\")\n",
    "\n",
    "path_imagesTr = join(nnunet_dir, \"nnUNet_base/nnUNet_raw/Task00_grid/imagesTr\")\n",
    "path_labelsTr = join(nnunet_dir, \"nnUNet_base/nnUNet_raw/Task00_grid/labelsTr\")\n",
    "path_imagesTs = join(nnunet_dir, \"nnUNet_base/nnUNet_raw/Task00_grid/imagesTs\")\n",
    "\n",
    "path_imagesTest = join(nnunet_dir, \"inference_test/input\")\n",
    "path_labelsTest = join(nnunet_dir, \"inference_test/labels\")\n",
    "path_outputsTest = join(nnunet_dir, \"inference_test/outputs\")\n",
    "\n",
    "path_imagesTr_rm = path_imagesTr + \"/*\"\n",
    "path_labelsTr_rm = path_labelsTr + \"/*\"\n",
    "path_imagesTs_rm = path_imagesTs + \"/*\"\n",
    "\n",
    "path_imagesTest_rm = path_imagesTest + \"/*\"\n",
    "path_labelsTest_rm = path_labelsTest + \"/*\"\n",
    "\n",
    "imagesTr = \"./imagesTr/\"\n",
    "labelsTr = \"./labelsTr/\"\n",
    "imagesTs = \"./imagesTs/\"\n",
    "\n",
    "path_model = join(nnunet_dir, \"nnUNet_base/nnUNet_training_output_dir/3d_fullres/Task00_grid/nnUNetTrainer__nnUNetPlans/all\")\n",
    "path_save_model = join(nnunet_dir, \"models\")\n",
    "\n",
    "dir_images_base = os.fsencode(path_images_base)\n",
    "dir_labels_base = os.fsencode(path_labels_base)\n",
    "\n",
    "# List all available images and labels\n",
    "\n",
    "list_images_base = []; list_labels_base = []\n",
    "\n",
    "for file in os.listdir(dir_images_base):\n",
    "     filename = os.fsdecode(file)\n",
    "     if filename.endswith(\".gz\"):\n",
    "         list_images_base.append(filename)\n",
    "         continue\n",
    "     else:\n",
    "         continue\n",
    "\n",
    "for file in os.listdir(dir_labels_base):\n",
    "     filename = os.fsdecode(file)\n",
    "     if filename.endswith(\".gz\"):\n",
    "         list_labels_base.append(filename)\n",
    "         continue\n",
    "     else:\n",
    "         continue\n",
    "\n",
    "list_images_base.sort()\n",
    "list_labels_base.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new directory for the model\n",
    "\n",
    "model_name = \"model_1_default_model\"\n",
    "\n",
    "model_dir = join(path_save_model, model_name)\n",
    "\n",
    "maybe_mkdir_p(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We attribute some percentages of the database to training, validation and testing\n",
    "\n",
    "tr_prop   = 0.70\n",
    "val_prop  = 0.00 # Training set includes cross validation (80:20)\n",
    "test_prop = 0.30\n",
    "\n",
    "samp_tr   = int(np.round(tr_prop   * len(list_images_base)))\n",
    "samp_val  = int(np.round(val_prop  * len(list_images_base)))\n",
    "samp_test = int(np.round(test_prop * len(list_images_base)))\n",
    "\n",
    "while samp_tr + samp_val + samp_test > len(list_images_base):\n",
    "    samp_test += -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We generate an order vector to shuffle the samples before each fold for the cross validation\n",
    "    \n",
    "order = np.arange(len(list_images_base))\n",
    "np.random.shuffle(order)\n",
    "\n",
    "list_images_base_fold = [list_images_base[i] for i in order]\n",
    "list_labels_base_fold = [list_labels_base[i] for i in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all files from previous fold\n",
    "\n",
    "files = glob.glob(path_imagesTr_rm)\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "files = glob.glob(path_labelsTr_rm)\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "files = glob.glob(path_imagesTs_rm)\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "files = glob.glob(path_imagesTest_rm)\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "files = glob.glob(path_labelsTest_rm)\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We generate an order vector to shuffle the samples before each fold for the cross validation\n",
    "\n",
    "list_imagesTr   = list_images_base_fold[0: samp_tr]\n",
    "list_labelsTr   = list_labels_base_fold[0: samp_tr]\n",
    "\n",
    "list_imagesTs   = list_images_base_fold[samp_tr: samp_tr + samp_val]\n",
    "list_labelsTs   = list_labels_base_fold[samp_tr: samp_tr + samp_val]\n",
    "\n",
    "list_imagesTest = list_images_base_fold[samp_tr + samp_val: samp_tr + samp_val + samp_test]\n",
    "list_labelsTest = list_labels_base_fold[samp_tr + samp_val: samp_tr + samp_val + samp_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift values in order for next fold of cross validation (a shift of samp_test)\n",
    "\n",
    "order = np.append(order[samp_test:], order[0:samp_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all corresponding files of present fold\n",
    "\n",
    "for ii in range(len(list_imagesTr)):\n",
    "    shutil.copyfile(path_images_base + \"/\" + list_imagesTr[ii],   path_imagesTr   + \"/\" + list_imagesTr[ii])\n",
    "\n",
    "for ii in range(len(list_labelsTr)):\n",
    "    shutil.copyfile(path_labels_base + \"/\" + list_labelsTr[ii],   path_labelsTr   + \"/\" + list_labelsTr[ii])\n",
    "\n",
    "for ii in range(len(list_imagesTs)):\n",
    "    shutil.copyfile(path_images_base + \"/\" + list_imagesTs[ii],   path_imagesTs   + \"/\" + list_imagesTs[ii])\n",
    "\n",
    "for ii in range(len(list_imagesTest)):\n",
    "    shutil.copyfile(path_images_base + \"/\" + list_imagesTest[ii], path_imagesTest + \"/\" + list_imagesTest[ii])\n",
    "\n",
    "for ii in range(len(list_labelsTest)):\n",
    "    shutil.copyfile(path_labels_base + \"/\" + list_labelsTest[ii], path_labelsTest + \"/\" + list_labelsTest[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the .json file for each fold of the cross validation\n",
    "\n",
    "list_imagesTr_json = [None] * len(list_imagesTr)\n",
    "list_labelsTr_json = [None] * len(list_labelsTr)\n",
    "list_imagesTs_json = [None] * len(list_imagesTs)\n",
    "\n",
    "for ii in range(len(list_imagesTr)):\n",
    "    list_imagesTr_json[ii] = imagesTr + list_imagesTr[ii]\n",
    "    list_labelsTr_json[ii] = labelsTr + list_labelsTr[ii]\n",
    "\n",
    "for ii in range(len(list_imagesTs)):\n",
    "    list_imagesTs_json[ii] = imagesTs + list_imagesTs[ii]\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "dataset = {\n",
    "    \"name\": \"StrokeVessels\",\n",
    "    \"description\": \"Upper Trunk Vessels Segmentation\",\n",
    "    \"reference\": \"Hospital Vall dHebron\",\n",
    "    \"licence\": \"-\",\n",
    "    \"release\": \"1.0 08/01/2020\",\n",
    "    \"tensorImageSize\": \"3D\",\n",
    "    \"modality\": {\n",
    "        \"0\": \"CT\"\n",
    "    },\n",
    "    \"labels\": {\n",
    "        \"0\": \"background\",\n",
    "        \"1\": \"vessel\"\n",
    "    },\n",
    "    \"numTraining\": samp_tr,\n",
    "    \"numTest\": samp_val,\n",
    "    \"training\": [],\n",
    "    \"test\": []\n",
    "}\n",
    "\n",
    "# We prepare the training and \"testing\" samples for the json file\n",
    "\n",
    "aux = []\n",
    "for ii in range(len(list_imagesTr_json)):\n",
    "    aux = np.append(aux, {\n",
    "                    \"image\": list_imagesTr_json[ii],\n",
    "                    \"label\": list_labelsTr_json[ii]\n",
    "                })\n",
    "\n",
    "aux = aux.tolist()\n",
    "\n",
    "aux2 = []\n",
    "for ii in range(len(list_imagesTs_json)):\n",
    "    aux2 = np.append(aux2, list_imagesTs_json[ii])\n",
    "\n",
    "if len(aux2) > 0:\n",
    "    aux2 = aux2.tolist()\n",
    "\n",
    "dataset[\"training\"] = aux\n",
    "dataset[\"test\"] = aux2\n",
    "\n",
    "with open(\"dataset.json\", \"w\") as outfile:\n",
    "    json.dump(dataset, outfile, indent=4)\n",
    "\n",
    "# Move json file to nnUNet_raw dir\n",
    "\n",
    "os.rename(nnunet_dir + \"/TFM/dataset.json\", nnunet_dir + \"/nnUNet_base/nnUNet_raw/Task00_grid/dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the json file ready, we shall begin preprocessing first:\n",
    "\n",
    "############### Plan and preprocessing ############\n",
    "\n",
    "# os.system(\"python3 experiment_planning/plan_and_preprocess_task.py -t Task00_grid -pl 4 -pf 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Training ####################\n",
    "\n",
    "# os.system(\"python3 run/run_training.py 3d_fullres nnUNetTrainer Task00_grid all --ndet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Inference ###################\n",
    "\n",
    "# os.system(\"python3 inference/predict_simple.py -i \" + path_imagesTest + \" -o \" + path_outputsTest + \" -t Task00_grid -tr nnUNetTrainer -m 3d_fullres -f all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.0005941390991210938 s\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-7f1c5df59b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Perform testing over inferred samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0meval_met\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0macc_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msen_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msen_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspe_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspe_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdice_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdice_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_met\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nnunet_env/nnUNet/nnunet/TFM/evaluation_metrics.py\u001b[0m in \u001b[0;36meval_metrics\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# y_out = t_out.numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;31m# dice = np.sum(y_out[y_label==1]==1)*2.0 / (np.sum(y_out==1) + np.sum(y_label==1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# dice_score.append(dice)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0munique_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munion1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munion1d\u001b[0;34m(ar1, ar2)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \"\"\"\n\u001b[0;32m--> 737\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Perform testing over inferred samples\n",
    "\n",
    "eval_met = eval_metrics()\n",
    "\n",
    "acc_mean, acc_std, sen_mean, sen_std, spe_mean, spe_std, dice_mean, dice_std = eval_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new file for the fold\n",
    "\n",
    "i = 0 # Remove this\n",
    "\n",
    "fold_dir = join(model_dir, \"fold\" + str(i))\n",
    "\n",
    "maybe_mkdir_p(fold_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model file and files for each fold and model\n",
    "\n",
    "for file in os.listdir(path_model):\n",
    "    filename = os.fsdecode(file)\n",
    "    os.rename(path_model + \"/\" + filename, fold_dir + \"/\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file = join(fold_dir, \"eval_metrics.csv\")\n",
    "\n",
    "np.savetxt(eval_file, eval_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fold\", str(i), \"for model\", model_name, \"complete. Evaluation metrics:\")\n",
    "print('accuracy =', acc_mean, acc_std)\n",
    "print('sensitivity =', sen_mean, sen_std)\n",
    "print('specificity =', spe_mean, spe_std)\n",
    "print('dice score =', dice_mean, dice_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}